
## 概率理论复习

> 1. 基础概念
2. 通用的离散分布
3. 通用的连续分布
4. 联合概率分布
5. 随机变量的变换
6. 蒙特卡罗近似
7. 信息论


### 1.基础概念

**离散随机变量`X`**, `X = x`表示随机事件`X`是`x`，而`p(x)`，又可以写作`p(X = x)`，表示其发生的概率。`p(x)`被称之为概率质量函数`PMF`

1. 事件A和事件B任意一件发生的概率为$p(A \vee B) = p(A) + p(B) - p(A \wedge B)$
2. 联合概率.
  1. 乘积规则: $p(A, B) = p(A \wedge B) = p(A|B)p(B)$
  2. 加法规则: 边缘概率$p(A) = \sum_b{p(A, B)}$ = \sum_b{p(A|B = b)p(B = b)}
  3. 多维联合概率: $p(X_{1:D}) = p(X_1)p(X_2|X_1)...p(X_D|X1:D-1)$
3. 条件概率: $p(A|B) = \frac{p(A, B)}{p(B)}$, 若$p(B) \lt 0$

**贝叶斯规则**

$$
p(X = x|Y = y) = \frac{p(X = x, Y = y)}{p(Y = y)} = \frac{p(X = x)p(Y = y | X = x)}{\sum_{x'}p(X = x')p(Y = y | X = x')}
$$

一个显著的例子便是测验和患病之间关系。假设测验的准确性是0.8，假阳性为0.1. 那么如果测验是阳性的话，是否就意味着你患病呢？ 这得看这种病的发生率了，假设人群中患病的比例是0.001

$$
\left{\begin{array}{R}
p(x = 试验阳性 | y = 患病) = 0.8 \\
p(x = 试验阳性 | y = 正常) = 0.1 \\
p(y = 患病) = 0.001
\end{array}\right
\implies p(y = 患病 | x = 试验阳性)
$$

**独立与条件独立**

`X`, `Y`无条件独立或者说边缘独立. $X \perp Y \iff p(X, Y) = p(X)p(Y)$

大部分的变量其实是受其他的变量的影响
$X \perp Y | Z \iff p(X, Y|Z) = p(X|Z)p(Y|Z)$

例如事件X: 明天下雨，Y：地上是湿的，Z：今天下雨

**连续随机变量**

事件`A(X <= a)`, `B(X <= b)`和`W(a< X <=b)`，则`F(q) = p(X <= a)`叫做累积分布函数(cdf)，它是一个**单调递增的函数**, 它的导数`F'(x)`则称为概率密度函数(pdf)，pdf的值可能是大于1的，一个比较明显的例子就是均匀分布(b = 1/2, a = 0)

cdf的反函数$F^{-1}(\alpha)$, $\alpha$就是事件发生的概率，可以用来刻画分布的均值什么的。

**均值和方差**

Mean: `E(X) = sum(x*f(x)) or integrate(x * f(x))`
Var : `V(X) = E( (X - E(X))**2 ) = E(X**2) - E(X)**2`

### 2.常见的离散分布

**Bernoulli**: 比如丢一次硬币的概率
**Binomial**: 比如丢n次硬币的概率
**Multi-Binomial**: 比如丢骰子
**Poisson**: Poi(x|a) = e^{-a} * a^x / x!, a > 0
**经验分布**: f(x) = { 1 if x in A else 0 }; D = { x1, x2, ... }; p(x) = sum( w * f(x) ) 约束在sum(w)=1


### 3.常见的连续分布

**高斯分布**: $N(x|\mu, \theta^2) = 1 / \sqrt(2 * pi * \theta^2) * exp^{-(x - \mu)^2 / (2*\theta^2)}$

用高斯分布的几个理由：
  1. 均值和方差好解释
  2. 中心极限定理说所有独立变量的和是接近高斯分布的，是计算误差的一个好模型
  3. 最少的假设-->看不懂

缺点是对于outlier比较敏感, 对应的有t分布来解决小样本带来这种问题，多了一个自由度

**Laplace分布**: Lap(x | u, b) = 1/(2 * b) * exp(-abs(x - u) / b), E(X) = u, var(X) = 2b^2

**gamma分布**: Ga(T | shape=a, rate=b) = b^a/gamma(a) * T^(a - 1) * e^(-Tb) { a, b > 0; gamma(x) = integrate(u^(x - 1) * e^(-x), 0, Inf)}

  E(X) = a / b; var = a / b^2

  Ga(x | 1, t) -> 指数分布
  Ga(x | 2, t) -> Erlang分布, 2可以是其他常整数
  Ga(x | v/2, 1/2)    -> Chi-Square分布，标准正态分布的随机变量的加和

  还有一个性质就是1 / X 属于逆gamma分布, mean = b / (a - 1); var = b^2 / (  ( a - 1 )^2 * (a - 2) ), a > 1, a > 2才分别有均值和方差

**beta分布**: Beta(x|a, b) = 1 / B(a, b) * x^(a - 1) * ( 1 - x )^(b - 1); B(a, b) = gamma(a) * gamma(b) / gamma(a + b), a, b > 0, x在0，1区间

**pareto分布**: Pareto(x|k, m) = k * m^k * x^(-k - 1)I(x >= m); 长尾分布

### 4. 联合概率分布

一般形式, p(x1, x2, ..., xn), n > 1, 模拟变量之间的相互关系, 如果不加限制的话，模型的复杂度在O(K^n), K是每个变量的状态数

**协方差和相关系数**

cov(X, Y) = E( (X - E(X))(Y - E(Y)) ) = E(XY) - E(X)E(Y)
cor(X, Y) = cov(X, Y) / sqrt(var(X) * var(Y))

独立的变量之间一定是相关性为0，但是相关性为0并不一定独立，例如Y=X^2

**多维高斯分布**: N(x|U, S) = 1 / [(2*pi)^(D/2) * det(S)^.5] * exp(-0.5 * (x - U)^T * S^-1 * (x - U)); S, D*D是协方差矩阵, 前面的系数同样是用来保证pdf能够积分到1

**多维t分布**

**Dirichlet分布**, Dir(X| A) 多维的beta分布泛化


### 5. 随机变量的转换

x ~ p() 随机变量， y = f(x), y的分布又是什么？

+ 线性转化
+ 基本转化
  离散相加
  连续积分, 转换为cdf然后求导
  变量的改变量 -> dy / dx 即为改变量


**中心极限定理**: 任何IID的随机变量相加转变后的变量是服从正太分布的

### 6. 蒙特卡罗近似

通过随机变量的改变量来求一个随机变量的分布是很难的，利用Monte Carlo方法产生样本，并用经验分布进行近似就可以近似估计该随机变量的分布

简单的例子, 已知x \in Uniform(-1, 1), y = x^2求y的概率分布


### 7. 信息熵

**熵**: H(X) = - \sum_{k=1}^{K}{p(X = k) * log2(p(X = k))}

**KL divergence相对熵**: KL(p||q) = sum(pk log(pk / qk)) = -H(p) + H(p, q), 将源信息解码出来需要的额外信息

**交叉熵**: H(p, q) = -sum(pk * log(qk))

**信息不等式**, KL(p||q) >= 0 仅当p = q时为0 => 证明需要用Jesen's 不等式

**互信息**: I(X; Y) = KL(p(X, Y) || P(X)P(Y)) = H(X) - H(X|Y) = H(Y) - H(X|Y)

条件熵H(X|Y)可以解释为观察到Y能够解释X能够减少多少的不可确定度
